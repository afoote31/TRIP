{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one file for Chapter 3, as there is really only one kind of visual, and most of the chapter is arguing the recurrences. For this file, the first two blocks involve loading the packages and defining some useful helper functions. For the simulation, make sure to set the `bootstrapping` variable properly and to uncomment the lines of code that correspond to the proper calculation of the recurrence. Also, change the line in the block that creates the visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generating functions\n",
    "def multivariateNorm(params):\n",
    "    muVec = np.zeros(params['dim'])\n",
    "    covMatrix = np.diag(np.ones(params['dim']))\n",
    "\n",
    "    covMatrix[0,1] = params['rho']\n",
    "    covMatrix[1,0] = params['rho']\n",
    "\n",
    "    X = np.random.multivariate_normal(muVec,covMatrix,params['sampleSize'])\n",
    "    epsilon = np.random.normal(0,0.1,params['sampleSize'])\n",
    "    y = np.sum(X,axis = 1) + epsilon\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def onlyDependent(params):\n",
    "    thetas = np.random.uniform(low = 0.0, high = 2*np.pi, size = params['sampleSize'])\n",
    "    x1 = np.sin(thetas)\n",
    "    x2 = np.cos(thetas)\n",
    "\n",
    "    otherVars = np.random.uniform(0,1,(params['dim']-2,params['sampleSize']))\n",
    "    \n",
    "    epsilon = np.random.normal(0,0.1,params['sampleSize'])\n",
    "\n",
    "    X = np.concatenate((np.array((x1,x2)).T,otherVars.T), axis = 1)\n",
    "    y = np.sum(X,axis = 1) + epsilon\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def blocks(params):\n",
    "    blockIdxStart = 2 # Want the first two features to be independent of everyone\n",
    "\n",
    "    covMatrix = np.zeros((params['dim'],params['dim']))\n",
    "    muVec = np.zeros(params['dim'])\n",
    "\n",
    "    while blockIdxStart +params['blockSize'] <= params['dim']:\n",
    "        covMatrix[blockIdxStart:blockIdxStart+params['blockSize'],blockIdxStart:blockIdxStart+params['blockSize']] = params['rho']\n",
    "        blockIdxStart += params['blockSize']\n",
    "    \n",
    "    for i in range(params['dim']):\n",
    "        covMatrix[i,i] = 1\n",
    "    \n",
    "    epsilon = np.random.normal(0,0.1,params['sampleSize'])\n",
    "\n",
    "    X = np.random.multivariate_normal(muVec,covMatrix,params['sampleSize'])\n",
    "    y = np.sum(X,axis = 1) + epsilon\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [0.01,0.05,0.25,0.75,1.0,1.5,2.0]\n",
    "Ts = [500,1000,1500,2000]\n",
    "ks = [1,3,5,15]\n",
    "repetitions = 10\n",
    "dataGenerators = [multivariateNorm,onlyDependent,blocks]\n",
    "variables = 10\n",
    "rho = 0.75\n",
    "bootstrapping = False # To recreate the analysis for bagged trees, set this to True\n",
    "dataGenDict = defaultdict(list)\n",
    "\n",
    "c = 1 - 1/np.e\n",
    "\n",
    "for gen in dataGenerators:\n",
    "    overallComputationsDict = defaultdict(list)\n",
    "    uniqueComputationsDict = defaultdict(list)\n",
    "    lDict = defaultdict(list)\n",
    "    recDict = defaultdict(list)\n",
    "    for T in tqdm(Ts):\n",
    "        for t in ts:\n",
    "            for k in ks:\n",
    "                for _ in range(repetitions):\n",
    "                    # Generate Data\n",
    "                    X,y = gen({'dim' : variables, 'sampleSize' : T, 'rho' : rho, 'blockSize' : 3})\n",
    "                    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.25, random_state=31)\n",
    "                    \n",
    "                    # Grow the forest\n",
    "                    forest = RandomForestRegressor(n_estimators = int(t*T), min_samples_leaf = k, n_jobs = -1,random_state = 31, max_features = 'sqrt',bootstrap = bootstrapping)\n",
    "                    forest.fit(xTrain, yTrain)\n",
    "\n",
    "                    trainObsInLeaves = defaultdict(list)\n",
    "                    trainingIndexList = forest.estimators_samples_ # indices of observations used in the fitting of each tree\n",
    "                    trees = forest.estimators_ # the decision tree structures\n",
    "\n",
    "                    for treeIdx,bootIndices in enumerate(trainingIndexList): # For each bootstrap tree...\n",
    "                        bootData = xTrain[bootIndices,] # Get the data used to fit that tree (including repeats)\n",
    "                        leafIndices = trees[treeIdx].apply(bootData) # Identify which leaf they end up in\n",
    "                        for idx,leafIdx in enumerate(leafIndices): # For each point in the bootstrap sample...\n",
    "                            trainObsInLeaves[(treeIdx,leafIdx)].append(bootIndices[idx]) # add its index to the leaf it ends up in in the tree\n",
    "\n",
    "                    # Calculate l\n",
    "                    lDict[(t,xTrain.shape[0],k)].append(np.mean([tree.get_n_leaves() for tree in forest.estimators_]))\n",
    "\n",
    "                    # Calculate the recurrence for leaf-weighted version (non-bagged trees)\n",
    "                    sumsOfSquaredTotalObs = []\n",
    "                    for i in range(int(t*T)):\n",
    "                        relevantKeys = [(i1,i2) for i1,i2 in trainObsInLeaves.keys() if i1 == i]\n",
    "                        acc = 0\n",
    "                        for key in relevantKeys:\n",
    "                            totalElsInLeaf = len(trainObsInLeaves[key])\n",
    "                            acc += totalElsInLeaf*totalElsInLeaf\n",
    "                        sumsOfSquaredTotalObs.append(acc)\n",
    "\n",
    "                    powElement = np.prod([1 - summation/((3*T/4)*(3*T/4)) for summation in sumsOfSquaredTotalObs])\n",
    "                    numerator = -(3*T/4)*(3*T/4)*powElement + (3*T/4)*(3*T/4)\n",
    "                    denominator = np.sum(sumsOfSquaredTotalObs)\n",
    "\n",
    "                    # Calculate the recurrence for leaf-weighted version (bagged trees)\n",
    "                    # sumsOfSquaredUniqueObs = []\n",
    "                    # sumsOfSquaredTotalObs = []\n",
    "                    # for i in range(int(t*T)):\n",
    "                    #     relevantKeys = [(i1,i2) for i1,i2 in trainObsInLeaves.keys() if i1 == i]\n",
    "                    #     denominatorAcc = 0\n",
    "                    #     numeratorAcc = 0\n",
    "                    #     for key in relevantKeys:\n",
    "                    #         totalElsInLeaf = len(trainObsInLeaves[key])\n",
    "                    #         denominatorAcc += totalElsInLeaf*totalElsInLeaf\n",
    "                    #         uniqueEls = np.unique(trainObsInLeaves[key]).shape[0]\n",
    "                    #         numeratorAcc += uniqueEls*uniqueEls\n",
    "                    #     sumsOfSquaredUniqueObs.append(numeratorAcc)\n",
    "                    #     sumsOfSquaredTotalObs.append(denominatorAcc)\n",
    "                    # powElement = np.prod([1 - summation/((3*T/4)*(3*T/4)*c) for summation in sumsOfSquaredUniqueObs])\n",
    "                    # numerator = -(3*T/4)*(3*T/4)*powElement + (3*T/4)*(3*T/4)\n",
    "                    # denominator = np.sum(sumsOfSquaredTotalObs)\n",
    "                    \n",
    "                    recDict[(t,xTrain.shape[0],k)].append(numerator/denominator)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    # For each data set, I now go through with my usual permutation hypothesis testing approach but now try the dynamic programming approach. Rather than \n",
    "                    # computing actual distances, I'm just going to tally the number of distances I would have had to compute\n",
    "                    overallComputations = 0\n",
    "                    uniqueComputations = 0\n",
    "                    \n",
    "                    xPerm = deepcopy(xTest)\n",
    "                    for col in range(xPerm.shape[1]): # For each column...\n",
    "                        np.random.shuffle(xPerm[:,col]) # shuffle the column\n",
    "                        leafIdxMatrix = forest.apply(xPerm)\n",
    "                        \n",
    "                        # our dynamic programming matrix; rows are test indices, columns training indices\n",
    "                        dpMatrix = np.zeros(shape = (xPerm.shape[0],xTrain.shape[0]))\n",
    "\n",
    "                        testObsInLeaves = defaultdict(list)\n",
    "                        for testIdx,obs in enumerate(leafIdxMatrix): # for each test point...\n",
    "                            for treeIdx,leafIdx in enumerate(obs): # for each leaf that test points ends up in...\n",
    "                                testObsInLeaves[(treeIdx,leafIdx)].append(testIdx) # accumulate\n",
    "\n",
    "                        # Get the indices of the leaves that test points end up in for each tree\n",
    "                        leafIdxDict = defaultdict(list)\n",
    "                        for i in range(leafIdxMatrix.shape[1]): # for each tree...\n",
    "                            # get the unique leaves reached by the test points\n",
    "                            leafIdxDict[str(i)] = np.unique(leafIdxMatrix[:,i])\n",
    "                        \n",
    "                        for treeIdx, leafIndices in leafIdxDict.items():\n",
    "                            for leafIdx in leafIndices:\n",
    "                                trainObsIndices = trainObsInLeaves[(int(treeIdx),leafIdx)]\n",
    "                                testObsIndices = testObsInLeaves[(int(treeIdx),leafIdx)]    \n",
    "\n",
    "                                for trainIdx in trainObsIndices:\n",
    "                                    for testIdx in testObsIndices:\n",
    "                                        # If you haven't computed the distance yet, mark that it was computed and count towards both unique and overall\n",
    "                                        if dpMatrix[testIdx,trainIdx] == 0:\n",
    "                                            overallComputations += 1\n",
    "                                            uniqueComputations += 1\n",
    "                                            dpMatrix[testIdx,trainIdx] = 1\n",
    "                                        # otherwise just increment overall computations\n",
    "                                        else:\n",
    "                                            overallComputations += 1\n",
    "                        xPerm[:,col] = xTest[:,col] # Change our permuted column back so that it doesn't mess up future iterations\n",
    "                    overallComputationsDict[(t,xTrain.shape[0],k)].append(overallComputations/xTest.shape[0]/xTest.shape[1]) # average computations per test observation, per feature\n",
    "                    uniqueComputationsDict[(t,xTrain.shape[0],k)].append(uniqueComputations/xTest.shape[0]/xTest.shape[1]) # average computations per training observation, per feature\n",
    "    dataGenDict[gen.__name__].append(overallComputationsDict)\n",
    "    dataGenDict[gen.__name__].append(uniqueComputationsDict)\n",
    "    dataGenDict[gen.__name__].append(lDict)\n",
    "    dataGenDict[gen.__name__].append(recDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['o','v','s','P']\n",
    "\n",
    "for k in dataGenDict.keys():\n",
    "    overall = dataGenDict[k][0]\n",
    "    unique = dataGenDict[k][1]\n",
    "    l = dataGenDict[k][2]\n",
    "    weightedRec = dataGenDict[k][3]\n",
    "\n",
    "    overallMeans = {k : np.mean(v) for k,v in overall.items()}\n",
    "    uniqueMeans = {k : np.mean(v) for k,v in unique.items()}\n",
    "    lMeans = {k : np.mean(v) for k,v in l.items()}\n",
    "    weightedRecMeans = {k : np.mean(v) for k,v in weightedRec.items()}\n",
    "\n",
    "\n",
    "    propUniqueSim = {k : uniqueMeans[k]/overallMeans[k] for k in overallMeans.keys()}\n",
    "\n",
    "    #propUniqueRec = {k : (-v*(1 - (1-1/np.e)/v)**(k[0]*k[1]*4/3) + v)/(k[0]*k[1]*4/3) for k,v in lMeans.items()} # For bagging\n",
    "    propUniqueRec = {k : (-v*(1 - 1/v)**(k[0]*k[1]*4/3) + v)/(k[0]*k[1]*4/3) for k,v in lMeans.items()} # For non-bagging\n",
    "    fig,axs = pyplot.subplots(ncols = len(Ts), figsize = (16,4), sharey = True)\n",
    "    for figIdx,T in enumerate(Ts):\n",
    "        for kIdx,msl in enumerate(ks):\n",
    "            instanceUniquePropSim = {k : v for k,v in propUniqueSim.items() if k[1] == T*0.75 and k[2] == msl} # 0.75 comes from the train-test split being 75-25\n",
    "            instanceUniquePropRec = {k : v for k,v in propUniqueRec.items() if k[1] == T*0.75 and k[2] == msl}\n",
    "            instanceUniquePropWeightedRec = {k : v for k,v in weightedRecMeans.items() if k[1] == T*0.75 and k[2] == msl}\n",
    "            axs[figIdx].plot(ts,instanceUniquePropSim.values(), marker = markers[kIdx], color = 'r',markersize = 7)\n",
    "            axs[figIdx].plot(ts,instanceUniquePropRec.values(), marker = markers[kIdx], color = 'b',markersize = 7)\n",
    "            axs[figIdx].plot(ts,instanceUniquePropWeightedRec.values(), marker = markers[kIdx], color = 'g', markersize = 7)\n",
    "            axs[figIdx].set(title = f'{int(T*0.75)} Train Obs.', ylabel = 'proportion of computations unique')\n",
    "            axs[figIdx].label_outer()\n",
    "    fig.supxlabel('trees (as proportion of data set size)',y = -0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
